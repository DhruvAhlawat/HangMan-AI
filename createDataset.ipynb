{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import LSTM\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think the solution lies in applying 3gram or 4gram NLP. Then we can check which is higher frequency and use that for predicting the next letter.\n",
    "def generateNgramOpcode(n,s):\n",
    "    curList = list(s); #split the string into a list of characters\n",
    "    if(n == 1):\n",
    "        return curList\n",
    "\n",
    "    #else what we will do is.\n",
    "    lowerNgramList = generateNgramOpcode(n-1, s)\n",
    "    retList = []\n",
    "    for curChar in curList:\n",
    "        for curNgram in lowerNgramList:\n",
    "            retList.append(curNgram+curChar)\n",
    "\n",
    "    return retList\n",
    "def addStartSymbol(s):\n",
    "    return ['^' + x for x in s]; \n",
    "def addEndSymbol(s):\n",
    "    return [x + '$' for x in s]; \n",
    "allAlphabets = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"words_250000_train.txt\",\"r\"); #since the original dataset was not\n",
    "TotalWordsList = text_file.read().splitlines(); \n",
    "trainWordsList, testWordsList = train_test_split(TotalWordsList, test_size=0.2, random_state=42); #only taking the first 200,000 words, so we can test on the remaining 50,000.\n",
    "text_file.close(); \n",
    "\n",
    "with open(\"trainWordsList.txt\", \"w\") as fp:   #Pickling\n",
    "    fp.writelines(\"%s\\n\" % word for word in trainWordsList) \n",
    "with open(\"testWordsList.txt\", \"w\") as fp:   #Pickling\n",
    "    fp.writelines(\"%s\\n\" % word for word in testWordsList)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for all of these we can simply store the frequency in a vector.\n",
    "text_file = open(\"trainWordsList.txt\",\"r\"); \n",
    "wordsList = text_file.read().splitlines(); \n",
    "text_file.close(); \n",
    "wordsList = [\"^\" + x + \"$\" for x in wordsList]; #add the start and end symbol\n",
    "TotalWordsList =  [\"^\" + x + \"$\" for x in TotalWordsList]; #add the start and end symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now in this wordsList we will try to store ngram stuff.\n",
    "#first we do 2gram analysis, then 3gram analysis.\n",
    "# #2gram analysis\n",
    "# #first we will create a list of all possible 2grams and 3grams.\n",
    "# all2grams = generateNgramOpcode(2, allAlphabets) + addStartSymbol(generateNgramOpcode(2, allAlphabets)) + addEndSymbol(generateNgramOpcode(2, allAlphabets)); #['^' + x for x in allAlphabets] + [x + '$' for x in allAlphabets]; \n",
    "# all4grams = generateNgramOpcode(4, allAlphabets) + addStartSymbol(generateNgramOpcode(3,allAlphabets)) + addEndSymbol(generateNgramOpcode(3,allAlphabets)) + addStartSymbol(addEndSymbol(generateNgramOpcode(2,allAlphabets))); #+ addEndSymbol(addStartSymbol(generateNgramOpcode(2,allAlphabets))); \n",
    "# all3grams = generateNgramOpcode(3, allAlphabets) + addStartSymbol(generateNgramOpcode(2, allAlphabets)) + addEndSymbol(generateNgramOpcode(2, allAlphabets)) + addStartSymbol(addEndSymbol(generateNgramOpcode(1,allAlphabets))); #+ addEndSymbol(addStartSymbol(generateNgramOpcode(2,allAlphabets)));\n",
    "# all5grams = generateNgramOpcode(5, allAlphabets) + addStartSymbol(generateNgramOpcode(4,allAlphabets)) + addEndSymbol(generateNgramOpcode(4,allAlphabets)) + addStartSymbol(addEndSymbol(generateNgramOpcode(3,allAlphabets))); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDict(n, l, name = None):\n",
    "    dictionary = {};\n",
    "    for curWord in l:\n",
    "        for i in range(len(curWord) - n + 1):\n",
    "            cur = curWord[i:i+n]; \n",
    "            if(cur not in dictionary):\n",
    "                dictionary[cur] = 0;\n",
    "            dictionary[cur] += 1; \n",
    "    if(name == None):\n",
    "        return dictionary; \n",
    "    with open(name+'.pickle', 'wb') as handle:\n",
    "        pickle.dump(dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)   \n",
    "    return dictionary;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDict(2, wordsList, 'dict2gram');\n",
    "getDict(3, wordsList, 'dict3gram');\n",
    "getDict(4, wordsList, 'dict4gram');\n",
    "getDict(5, wordsList, 'dict5gram');\n",
    "getDict(2, TotalWordsList, 'finaldict2gram');\n",
    "getDict(3, TotalWordsList, 'finaldict3gram');\n",
    "getDict(4, TotalWordsList, 'finaldict4gram');\n",
    "getDict(5, TotalWordsList, 'finaldict5gram');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDict(6, wordsList, 'dict6gram'); \n",
    "getDict(7, wordsList, 'dict7gram');\n",
    "getDict(8, wordsList, 'dict8gram');\n",
    "getDict(9, wordsList, 'dict9gram');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
